{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D U-Net Model Evaluation for Lung CT Image Segmentation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates the evaluation of a 3D U-Net model used for segmenting lung structures from 3D CT scans.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Model Architecture**: Utilizes a 3D U-Net network.\n",
    "- **Evaluation**: Includes setup for evaluating model performance\n",
    "- **Disease Map Outputs**: Generates disease map outputs for visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "spec = 5\n",
    "# select test(1) or training(0)\n",
    "test = 0\n",
    "\n",
    "batch_size = 1\n",
    "number_of_epochs = 100\n",
    "filename = '_defEx_InspMask2' #'_DIV_OK' # _DIV_OK\n",
    "\n",
    "# save csv metrics and disease maps\n",
    "save = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd /data-synology/anlee/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 435,
     "status": "ok",
     "timestamp": 1691536112346,
     "user": {
      "displayName": "Thomas Keller",
      "userId": "17223430032597201940"
     },
     "user_tz": 420
    },
    "id": "vgh3OIul1RvA",
    "outputId": "e2130622-09d6-4496-d403-59e9c73ad2ae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4870,
     "status": "ok",
     "timestamp": 1691536123960,
     "user": {
      "displayName": "Thomas Keller",
      "userId": "17223430032597201940"
     },
     "user_tz": 420
    },
    "id": "5_BvSxkr3BBw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# includes & imports\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" #for CPU \"\"\n",
    "\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndi\n",
    "#import nilearn as nil\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# random seeds\n",
    "np.random.seed(16)\n",
    "tf.random.set_seed(16)\n",
    "tf.keras.utils.set_random_seed(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mhg2AoOLcAf"
   },
   "source": [
    "Build an input pipeline with image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31949,
     "status": "ok",
     "timestamp": 1689621403431,
     "user": {
      "displayName": "Thomas Keller",
      "userId": "17223430032597201940"
     },
     "user_tz": 420
    },
    "id": "Szib1HgsLasf",
    "outputId": "35cd0da8-264b-4d44-b2bc-696a97668cb6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get image pathes for input and target images\n",
    "from tqdm import tqdm\n",
    "\n",
    "root_directory = '/data-synology/anlee/COPDGene/'  # Replace with the actual path to your root directory\n",
    "inputImageName = ('insp_ct_ds.nii') #('insp_ct.nii')\n",
    "outputImageName = ('exp_ct_deform_ds.nii')  # Extensions of the target image files\n",
    "maskInspImageName = ('insp_mask_cat.nii')  # Mask image\n",
    "#maskDefExpImageName = ('exp_mask_deform_cat.nii')  # Mask image\n",
    "\n",
    "\n",
    "def search_images(directory, image_list, name):\n",
    "  for root, dirs, files in tqdm(os.walk(directory)):\n",
    "        for file in files:\n",
    "            if file.endswith(name):\n",
    "                image_path = os.path.join(root, file)\n",
    "                image_list.append(image_path.replace('\\0', ''))  # Add the image path to the list, replace termination character\n",
    "\n",
    "# Create an empty list to store image paths\n",
    "inputImagePath = []\n",
    "outputImagePath = []\n",
    "maskInspImagePath = []\n",
    "#maskDefExpImagePath = []\n",
    "\n",
    "# Call the search_image function with the root directory\n",
    "search_images(root_directory, inputImagePath, inputImageName)\n",
    "search_images(root_directory, outputImagePath, outputImageName)\n",
    "search_images(root_directory, maskInspImagePath, maskInspImageName)\n",
    "#search_images(root_directory, maskDefExpImagePath, maskDefExpImageName)\n",
    "\n",
    "# check if path loaded\n",
    "if inputImagePath:\n",
    "    print(\"Image input paths:\")\n",
    "    for path in inputImagePath[:5]:\n",
    "        print(path)\n",
    "else:\n",
    "    print(\"No image files found in the directory tree.\")\n",
    "\n",
    "if outputImagePath:\n",
    "    print(\"Image ouput paths:\")\n",
    "    for path in outputImagePath[:5]:\n",
    "        print(path)\n",
    "else:\n",
    "    print(\"No image files found in the directory tree.\")\n",
    "    \n",
    "if maskInspImagePath:\n",
    "    print(\"Image mask insp paths:\")\n",
    "    for path in maskInspImagePath[:5]:\n",
    "        print(path)\n",
    "else:\n",
    "    print(\"No image files found in the directory tree.\")\n",
    "    \n",
    "#if maskDefExpImagePath:\n",
    "#    print(\"Image mask defExp paths:\")\n",
    "#    for path in maskDefExpImagePath[:5]:\n",
    "#        print(path)\n",
    "#else:\n",
    "#    print(\"No image files found in the directory tree.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1689621403984,
     "user": {
      "displayName": "Thomas Keller",
      "userId": "17223430032597201940"
     },
     "user_tz": 420
    },
    "id": "HiHMHEnb-r12",
    "outputId": "f7eb6796-8c1c-4eac-d482-3f5d286d959d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert image paths to lists\n",
    "inputImagePath = list(inputImagePath)\n",
    "outputImagePath = list(outputImagePath)\n",
    "maskInspImagePath = list(maskInspImagePath)\n",
    "#maskDefExpImagePath = list(maskDefExpImagePath)\n",
    "\n",
    "# Split the data into training and test sets list\n",
    "train_input, test_input, train_output, test_output, train_insp_mask, test_insp_mask = train_test_split(\n",
    "    inputImagePath, outputImagePath, maskInspImagePath, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the data into test and validation sets list\n",
    "val_input, test_input, val_output, test_output, val_insp_mask, test_insp_mask = train_test_split(\n",
    "    test_input, test_output, test_insp_mask, test_size=0.5, random_state=42)\n",
    "\n",
    "print('training data: ' + str(len(train_input)))\n",
    "print('validation data: ' + str(len(val_input)))\n",
    "print('test data: ' + str(len(test_input)))\n",
    "print('train mask inspiratory data: ' + str(len(train_insp_mask)))\n",
    "print('test mask inspiratory data: ' + str(len(test_insp_mask)))\n",
    "#print('train mask expiratory data: ' + str(len(train_exp_mask)))\n",
    "#print('test mask expiratory data: ' + str(len(test_exp_mask)))\n",
    "\n",
    "if train_input:\n",
    "    print(\"Image input paths:\")\n",
    "    for path in train_input[:5]:\n",
    "        print(path)\n",
    "else:\n",
    "    print(\"No image files found in the directory tree.\")\n",
    "\n",
    "if train_output:\n",
    "    print(\"Image ouput paths:\")\n",
    "    for path in train_output[:5]:\n",
    "        print(path)\n",
    "else:\n",
    "    print(\"No image files found in the directory tree.\")\n",
    "    \n",
    "if train_insp_mask:\n",
    "    print(\"Image mask inspiratory paths:\")\n",
    "    for path in train_insp_mask[:5]:\n",
    "        print(path)\n",
    "else:\n",
    "    print(\"No image files found in the directory tree.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Trc3urJsKrYI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load image function to load images scaled by 3000 (HU)\n",
    "def load_image_scale(file_path):\n",
    "    \n",
    "    # load nibable image\n",
    "    image_data = nib.load(file_path).get_fdata()\n",
    "\n",
    "    # pick middle slice\n",
    "    image = image_data\n",
    "\n",
    "    # convert values\n",
    "    image = (image / 3000)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image function to load images not scaled\n",
    "def load_image(file_path):\n",
    "\n",
    "    # load nibable image\n",
    "    image_data = nib.load(file_path).get_fdata()\n",
    "    \n",
    "    # pick middle slice\n",
    "    image = image_data\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image function to load image mask\n",
    "def load_image_mask(file_path):\n",
    "\n",
    "    # load nibable image\n",
    "    image_data = nib.load(file_path).get_fdata()\n",
    "\n",
    "    # pick middle slice\n",
    "    mask = image_data\n",
    "\n",
    "    # create binary image mask\n",
    "    binary_mask_slice = np.where((mask > 0) & (mask < 6), 1, 0)\n",
    "    \n",
    "    return binary_mask_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select test or train\n",
    "if test == 1:\n",
    "    test_input_smaller = test_input[0::128]\n",
    "    test_output_smaller = test_output[0::128] \n",
    "    test_mask_insp_smaller = test_insp_mask[0::128]\n",
    "\n",
    "\n",
    "    test_dataset_paths = (test_input_smaller, test_output_smaller)\n",
    "    test_mask_insp_paths = test_mask_insp_smaller\n",
    "\n",
    "    \n",
    "else:\n",
    "    train_dataset_paths = (train_input, train_output)\n",
    "\n",
    "    test_dataset_paths = (test_input, test_output)\n",
    "    test_mask_insp_paths = test_insp_mask\n",
    "\n",
    "# calucalte steps and so on\n",
    "length = len(test_dataset_paths[0])\n",
    "print('length test:')\n",
    "print(length)\n",
    "\n",
    "# calculate steps per epoch and validation steps\n",
    "steps_per_epoch = len(test_dataset_paths[0]) // batch_size\n",
    "print('steps_per_epoch: ' + str(steps_per_epoch))\n",
    "\n",
    "test_steps = len(test_dataset_paths[0]) // batch_size\n",
    "print('test_steps: ' + str(test_steps))\n",
    "\n",
    "# calculate the number of training iterations\n",
    "number_of_steps_total = int(steps_per_epoch * number_of_epochs)\n",
    "print('number_of_steps_total: ' + str(number_of_steps_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# data generator v1\n",
    "# Output: yield np.array(test_input_images), [np.array(test_output_images),np.array(test_output_mask)]\n",
    "\n",
    "\n",
    "def data_generator(paths, mask_insp_paths, batch_size):\n",
    "    test_input_paths, test_output_paths = paths\n",
    "    \n",
    "    num_samples = len(test_input_paths)\n",
    "    #print(num_samples)\n",
    "    indices = list(range(num_samples))\n",
    "        \n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_indices = indices[i:i+batch_size]\n",
    "\n",
    "        #print(batch_indices)\n",
    "        \n",
    "        test_input_images = []\n",
    "        test_output_images = []\n",
    "        test_output_mask = []\n",
    "            \n",
    "        for idx in batch_indices:\n",
    "            \n",
    "            # load mask\n",
    "            mask = load_image_mask(mask_insp_paths[idx])\n",
    "            # load input image\n",
    "            insp_image = load_image_scale(test_input_paths[idx])\n",
    "            # mask input image\n",
    "            input_image = insp_image * mask\n",
    "            # load deformed exp image\n",
    "            defexp_image = load_image(test_output_paths[idx])\n",
    "            # calculate subtraction and mask it\n",
    "            output_image = defexp_image * mask\n",
    "            # subtraction image\n",
    "            diseas_image = (output_image - insp_image)\n",
    "            diseas_mask = np.where(diseas_image < 1/30 , 1, 0) * mask # check\n",
    "            \n",
    "            # append images to input and output train\n",
    "            test_input_images.append(np.expand_dims(input_image, -1))\n",
    "            test_output_images.append(np.expand_dims(output_image, -1))\n",
    "            test_output_mask.append(np.expand_dims(diseas_mask, -1))\n",
    "            #test_output_mask.append(np.expand_dims(mask, -1))\n",
    "\n",
    "        \n",
    "        yield np.array(test_input_images), [np.array(test_output_images),np.array(test_output_mask)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZAc3uLSjLkB"
   },
   "source": [
    "Image generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data generators for test and validation\n",
    "test_generator = data_generator(test_dataset_paths, test_mask_insp_paths, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = data_generator(test_dataset_paths, test_mask_insp_paths, batch_size)\n",
    "inputs, (outputs, outputs2) = next(gen)\n",
    "\n",
    "print(inputs.shape, outputs.shape, outputs2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.rot90(outputs2[0,96, :, :,0], 1), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.colorbar(orientation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inp, out = next(test_generator)\n",
    "#print(inp.shape, out.shape)\n",
    "print(number_of_steps_total)\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_steps_total = 638000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred, threshold=100, scale=3000):\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    \n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + epsilon\n",
    "    \n",
    "    dice = (2. * intersection + epsilon) / union\n",
    "    return (1 - dice)  # For loss, return 1 - dice to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_path = f'/data-synology/tkeller/Outputs/3D_nsteps{number_of_steps_total}_batch{batch_size}_DiceLoss_{spec}/'\n",
    "\n",
    "# Load the model with custom_objects\n",
    "model = load_model(model_path, custom_objects={\n",
    "    'dice_coefficient': dice_coefficient})\n",
    "\n",
    "print('Model loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust calc functions and why do we predict the mask/??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualte MSE\n",
    "def compute_mse(ground_truth, prediction):\n",
    "    mse = np.mean((ground_truth - prediction) ** 2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte airtrapping\n",
    "def compute_airtrapping_per_image(inspiratory, defExp_prediction_image, mask):\n",
    "\n",
    "    subtraction_image = (defExp_prediction_image - inspiratory) * mask\n",
    "    \n",
    "    sub_prediction_image = subtraction_image * 3000\n",
    "\n",
    "    threshold = 100\n",
    "\n",
    "    # count values below threshold\n",
    "    airtrapping_voxels = np.sum((sub_prediction_image < threshold) & (sub_prediction_image != 0))\n",
    "    \n",
    "    # count total total values\n",
    "    total_voxels = np.count_nonzero(sub_prediction_image)\n",
    "\n",
    "    if total_voxels > 0:\n",
    "        percentage_airtrapping = (airtrapping_voxels / total_voxels) * 100\n",
    "    else:\n",
    "        percentage_airtrapping = 0\n",
    "    \n",
    "    return percentage_airtrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dice score\n",
    "# input: deformed expiratory, inpsriatory, predicted\n",
    "def calculate_dice_score(inspiratory, output, masked_predictions, mask):\n",
    "\n",
    "    threshold = 100\n",
    "    \n",
    "    # defExp - inspiratory\n",
    "    sub_real = (output - inspiratory)\n",
    "    mt = np.where(sub_real * 3000 < threshold, 1, 0) * mask\n",
    "\n",
    "    # defExp predict - inspiratory\n",
    "    sub_fake = (masked_predictions - inspiratory)\n",
    "    mp = np.where(sub_fake * 3000 < threshold, 1, 0) * mask\n",
    "    \n",
    "    dice_value = (2 * np.sum(mt * mp)) / (np.sum(mt + mp))\n",
    "    \n",
    "    return dice_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# plot Input image, Predicted image, Predicted mask, and save\n",
    "def plot_predictions_with_masks(batch_input, image_predictions, mask_predictions, counter, index=0):\n",
    "\n",
    "    save_dir = f'/data-synology/tkeller/Outputs/3D_nsteps{number_of_steps_total}_batch{batch_size}_DiceLoss_{spec}/outputsEx'\n",
    "    \n",
    "    input_image = batch_input[index]\n",
    "    predicted_image = image_predictions[index]\n",
    "    predicted_mask = mask_predictions[index]\n",
    "\n",
    "    middle_slice = input_image.shape[0] // 2  \n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.imshow(np.rot90(input_image[middle_slice, :, :], 1), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.colorbar(orientation='vertical')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Predicted Image\")\n",
    "    plt.imshow(np.rot90(predicted_image[middle_slice, :, :], 1), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.colorbar(orientation='vertical')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.imshow(np.rot90(predicted_mask[middle_slice, :, :], 1), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.colorbar(orientation='vertical')\n",
    "\n",
    "    # Check if save directory exists, if not, create it\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(os.path.join(save_dir, f'prediction_{counter}.png'), dpi=300)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable for counting plots\n",
    "counter = 0\n",
    "\n",
    "# plot plot predictions with masks\n",
    "for batch_input, batch_output in test_generator:\n",
    "    image_predictions, mask_predictions = model.predict(batch_input)\n",
    "    plot_predictions_with_masks(batch_input, image_predictions, mask_predictions, counter)\n",
    "    counter += 1\n",
    "    if counter == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# data generator v2\n",
    "# Output: yield np.array(test_input_images), np.array(test_output_images),np.array(test_output_mask)\n",
    "\n",
    "def data_generator2(paths, mask_insp_paths, batch_size):\n",
    "    test_input_paths, test_output_paths = paths\n",
    "    \n",
    "    num_samples = len(test_input_paths)\n",
    "    #print(num_samples)\n",
    "    indices = list(range(num_samples))\n",
    "        \n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_indices = indices[i:i+batch_size]\n",
    "\n",
    "        #print(batch_indices)\n",
    "        \n",
    "        test_input_images = []\n",
    "        test_output_images = []\n",
    "        test_output_mask = []\n",
    "            \n",
    "        for idx in batch_indices:\n",
    "            \n",
    "            # load mask\n",
    "            mask = load_image_mask(mask_insp_paths[idx])\n",
    "            # load input image\n",
    "            insp_image = load_image_scale(test_input_paths[idx])\n",
    "            # mask input image\n",
    "            input_image = insp_image * mask\n",
    "            # load deformed exp image\n",
    "            defexp_image = load_image(test_output_paths[idx])\n",
    "            # calculate subtraction and mask it\n",
    "            output_image = defexp_image * mask\n",
    "\n",
    "            #diseas_image = (output_image - insp_image)\n",
    "            #diseas_mask = np.where(diseas_image < 1/30 , 1, 0) * mask # check\n",
    "            \n",
    "            # append images to input and output train\n",
    "            test_input_images.append(np.expand_dims(input_image, -1))\n",
    "            test_output_images.append(np.expand_dims(output_image, -1))\n",
    "            #test_output_mask.append(np.expand_dims(diseas_mask, -1))\n",
    "            test_output_mask.append(np.expand_dims(mask, -1))\n",
    "\n",
    "                yield np.array(test_input_images), [np.array(test_output_images),np.array(test_output_mask)]\n",
    "\n",
    "        yield np.array(test_input_images), np.array(test_output_images),np.array(test_output_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data generator 2\n",
    "test_generator2 = data_generator2(test_dataset_paths, test_mask_insp_paths, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjvPWbxcL17j",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize placeholders for statistics\n",
    "\n",
    "mse_values = []\n",
    "airtrapping_percentages_fake = []\n",
    "airtrapping_percentages_real = []\n",
    "dice_scores = []\n",
    "\n",
    "\n",
    "for batch_input, batch_output, batch_masks in tqdm(test_generator2, total=test_steps, desc=\"Processing batches\"):\n",
    "\n",
    "    # predict\n",
    "    #predictions = model(batch_input, training=False)\n",
    "\n",
    "    predictions, mask_predictionsX = model(batch_input, training=False)\n",
    "\n",
    "    masked_predictions = []\n",
    "    \n",
    "    # maske predictions\n",
    "    for prediction, mask in zip(predictions,batch_masks):\n",
    "        mask_pred = prediction * mask\n",
    "        masked_predictions.append(mask_pred)\n",
    "\n",
    "    # calculate MSE\n",
    "    for defExp, prediction in zip(batch_output, masked_predictions):\n",
    "        mse_values.append(compute_mse(defExp, prediction))\n",
    "\n",
    "    # Iterate through the lists and calculate airtrapping percentages\n",
    "    for inspiratory, prediction, mask in zip(batch_input, masked_predictions, batch_masks):\n",
    "        airtrapping_percentages_fake.append(compute_airtrapping_per_image(inspiratory, prediction, mask))\n",
    "\n",
    "    # Iterate through the lists and calculate airtrapping percentages\n",
    "    for inspiratory, subtraction, mask in zip(batch_input, batch_output, batch_masks):\n",
    "        airtrapping_percentages_real.append(compute_airtrapping_per_image(inspiratory, subtraction, mask))\n",
    "\n",
    "    # Iterate through the lists and calculate dice scores\n",
    "    for inspiratory, output, predictions, mask in zip(batch_input, batch_output, masked_predictions, batch_masks):\n",
    "        dice_scores.append(calculate_dice_score(inspiratory, output, predictions, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data generator 3\n",
    "test_generator3 = data_generator2(test_dataset_paths, test_mask_insp_paths, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize a list to store overall air trapping information for 3D images\n",
    "overall_info_3d_images = []\n",
    "\n",
    "for batch_idx, (batch_input, batch_output, batch_masks) in enumerate(tqdm(test_generator3, total=test_steps, desc=\"Processing batches\")):\n",
    "    predictions, mask_predictions = model(batch_input, training=False)\n",
    "    \n",
    "    for item_idx in range(batch_input.shape[0]):  # Iterate through each 3D image in the batch\n",
    "        \n",
    "        # Compute air trapping for the entire 3D image (all slices)\n",
    "        overall_true_airtrapping = compute_airtrapping_per_image(batch_input[item_idx], batch_output[item_idx], batch_masks[item_idx])\n",
    "        overall_predicted_airtrapping = compute_airtrapping_per_image(batch_input[item_idx], predictions[item_idx], batch_masks[item_idx])\n",
    "\n",
    "        # Initialize variables to find the worst slice\n",
    "        worst_difference = 0\n",
    "        worst_slice_idx = None\n",
    "        worst_true_airtrapping_slice = 0\n",
    "        worst_predicted_airtrapping_slice = 0\n",
    "\n",
    "        # Iterate over each slice to find the one with the worst air trapping difference\n",
    "        for slice_idx in range(batch_input.shape[1]):  \n",
    "            inspiratory_slice = batch_input[item_idx, slice_idx, :, :]\n",
    "            prediction_slice = predictions[item_idx, slice_idx, :, :]\n",
    "            mask_slice = batch_masks[item_idx, slice_idx, :, :]\n",
    "\n",
    "            # Calculate air trapping for each slice\n",
    "            true_airtrapping_slice = compute_airtrapping_per_image(inspiratory_slice, batch_output[item_idx, slice_idx, :, :], mask_slice)\n",
    "            predicted_airtrapping_slice = compute_airtrapping_per_image(inspiratory_slice, prediction_slice, mask_slice)\n",
    "\n",
    "            # Calculate the difference in air trapping between true and prediction for the slice\n",
    "            difference = abs(true_airtrapping_slice - predicted_airtrapping_slice)\n",
    "\n",
    "            # Update worst slice if current slice difference is greater\n",
    "            if difference > worst_difference:\n",
    "                worst_difference = difference\n",
    "                worst_slice_idx = slice_idx\n",
    "                worst_true_airtrapping_slice = true_airtrapping_slice\n",
    "                worst_predicted_airtrapping_slice = predicted_airtrapping_slice\n",
    "\n",
    "        # Store information for the 3D image including overall air trapping and the worst slice\n",
    "        overall_info_3d_images.append({\n",
    "            'batch_idx': batch_idx,\n",
    "            'item_idx': item_idx,\n",
    "            'overall_true_airtrapping': overall_true_airtrapping,\n",
    "            'overall_predicted_airtrapping': overall_predicted_airtrapping,\n",
    "            'worst_slice_idx': worst_slice_idx,\n",
    "            'worst_difference': worst_difference,\n",
    "            'worst_true_airtrapping_slice': worst_true_airtrapping_slice,\n",
    "            'worst_predicted_airtrapping_slice': worst_predicted_airtrapping_slice,\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the overall difference between true and predicted air trapping for each 3D image\n",
    "for image_detail in overall_info_3d_images:\n",
    "    overall_true = image_detail.get('overall_true_airtrapping', 0)\n",
    "    overall_predicted = image_detail.get('overall_predicted_airtrapping', 0)\n",
    "    overall_difference = abs(overall_true - overall_predicted)\n",
    "    image_detail['overall_difference'] = overall_difference\n",
    "\n",
    "# Sort images by their overall difference in descending order\n",
    "sorted_images_by_overall_difference = sorted(overall_info_3d_images, key=lambda x: x['overall_difference'], reverse=True)\n",
    "\n",
    "# Extract the top images with the largest overall difference\n",
    "images_with_largest_overall_difference = sorted_images_by_overall_difference[:20]  # Adjust the number as needed\n",
    "\n",
    "# Report the images with the largest overall difference\n",
    "for image in images_with_largest_overall_difference:\n",
    "    print(f\"Batch {image['batch_idx']}, Image {image['item_idx']}:\")\n",
    "    print(f\"  Overall True Air Trapping: {image.get('overall_true_airtrapping', 'N/A')}%\")\n",
    "    print(f\"  Overall Predicted Air Trapping: {image.get('overall_predicted_airtrapping', 'N/A')}%\")\n",
    "    print(f\"  Worst Slice Index: {image.get('worst_slice_idx', 'N/A')}\")\n",
    "    print(f\"  Worst Slice True Air Trapping: {image.get('worst_true_airtrapping_slice', 'N/A')}%\")\n",
    "    print(f\"  Worst Slice Predicted Air Trapping: {image.get('worst_predicted_airtrapping_slice', 'N/A')}%\")\n",
    "    print(f\"  Overall Difference: {image.get('overall_difference', 'N/A')}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_batches = set(img['batch_idx'] for img in images_with_largest_overall_difference)\n",
    "print(desired_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator4 = data_generator2(test_dataset_paths, test_mask_insp_paths, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for image in images_with_largest_overall_difference:\n",
    "#    print(f\"Batch {image['batch_idx']}, Image {image['item_idx']}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Initialize the color maps\n",
    "insp_map = ListedColormap(cm.get_cmap('Reds', 256)(np.linspace(1, 0, 256)))\n",
    "diff_map = ListedColormap(cm.get_cmap('Blues', 256)(np.linspace(1, 0, 256)))\n",
    "\n",
    "save_dir = (f\"/home/tkeller/google-drive/LossPlots/Worst_Predcitions_DiceLoss_{spec}/\")\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Reset the generator and set the current batch index\n",
    "current_batch_idx = -1\n",
    "\n",
    "for batch_input, batch_output, batch_masks in test_generator4:\n",
    "    current_batch_idx += 1\n",
    "\n",
    "    if current_batch_idx not in desired_batches:\n",
    "        continue\n",
    "\n",
    "    filtered_images = [img for img in images_with_largest_overall_difference if img['batch_idx'] == current_batch_idx]\n",
    "\n",
    "    for image_details in filtered_images:\n",
    "        batch_idx = image_details['batch_idx']  # Use 'image_details' instead of 'image_detail'\n",
    "        item_idx = image_details['item_idx']\n",
    "        worst_slice_idx = image_details['worst_slice_idx']\n",
    "        worst_true_airtrapping = image_details['worst_true_airtrapping_slice']\n",
    "        true_airtrapping_overall = image_details['overall_true_airtrapping']\n",
    "        worst_predicted_airtrapping = image_details['worst_predicted_airtrapping_slice']   \n",
    "        predicted_airtrapping_overall = image_details['overall_predicted_airtrapping']\n",
    "        predictions, mask_pred = model.predict(batch_input)\n",
    "\n",
    "        # Processing images and masks for visualization\n",
    "        insp_ct_ds = np.rot90(np.squeeze(batch_input[item_idx, worst_slice_idx, :, :] * 3000))\n",
    "        prediction_sub = np.rot90(np.squeeze(predictions[item_idx, worst_slice_idx, :, :] - batch_input[item_idx, worst_slice_idx, :, :]) * 3000)\n",
    "        insp_whole = np.rot90(np.squeeze(batch_masks[item_idx, worst_slice_idx, :, :]))\n",
    "        mask_pred = np.rot90(np.squeeze(mask_pred[item_idx, worst_slice_idx, :, :]))\n",
    "\n",
    "        \n",
    "        # Apply Gaussian filter to smooth the images for better visualization\n",
    "        insp_slc_mask_smooth = gaussian_filter(insp_ct_ds, sigma=1) * insp_whole\n",
    "        diff_slc_pred_mask_smooth = np.abs(gaussian_filter(prediction_sub, sigma=1)) * insp_whole\n",
    "\n",
    "        plt.figure(figsize=(24, 10))\n",
    "\n",
    "        # Display the ground truth image with airtrapping percentage\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.imshow(insp_ct_ds, cmap='gray', vmin=-400-1500, vmax=-400+1500)\n",
    "        plt.imshow(np.ma.masked_where(insp_whole == 0, insp_slc_mask_smooth), cmap=insp_map, alpha=0.5)\n",
    "        plt.title(f\"Ground Truth - Batch: {current_batch_idx}, Slice: {worst_slice_idx}\\nTrue Air Trapping: {worst_true_airtrapping:.2f}%\", fontsize=12)\n",
    "        plt.axis('off')\n",
    "        plt.colorbar(orientation='vertical')\n",
    "\n",
    "\n",
    "        # Display the prediction image with airtrapping percentage\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.imshow(prediction_sub, cmap='gray', vmin=-400-1500, vmax=-400+1500)\n",
    "        plt.imshow(np.ma.masked_where(insp_whole == 0, diff_slc_pred_mask_smooth), cmap=diff_map, alpha=0.5)\n",
    "        plt.title(f\"Prediction - Batch: {current_batch_idx}, Slice: {worst_slice_idx}\\nPredicted Air Trapping: {worst_predicted_airtrapping:.2f}%\", fontsize=12)\n",
    "        plt.axis('off')\n",
    "        plt.colorbar(orientation='vertical')\n",
    "\n",
    "\n",
    "        # Mask only\n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.imshow(mask_pred, cmap='jet')\n",
    "        plt.title(f\"Mask Only - Batch: {current_batch_idx}, Slice: {worst_slice_idx}\", fontsize=12)\n",
    "        plt.axis('off')\n",
    "        plt.colorbar(orientation='vertical')\n",
    "\n",
    "\n",
    "        # Histogram for the ground truth pixel values\n",
    "        plt.subplot(2, 3, 4)\n",
    "        gt_values = insp_ct_ds[insp_whole != 0].flatten()\n",
    "        plt.hist(gt_values, bins=50, color='purple', alpha=0.7)\n",
    "        plt.axvline(x=100, color='k', linestyle='--', label='100 HU')  \n",
    "        plt.axvline(x=-860, color='k', linestyle='--', label='-860 HU')  \n",
    "    \n",
    "        plt.title(\"Ground Truth Histogram\", fontsize=12)\n",
    "\n",
    "        # Histogram for the prediction pixel values\n",
    "        plt.subplot(2, 3, 5)\n",
    "        pred_values = prediction_sub[insp_whole != 0].flatten()\n",
    "        plt.hist(pred_values, bins=50, color='purple', alpha=0.7)\n",
    "        plt.axvline(x=100, color='k', linestyle='--', label='100 HU') \n",
    "        plt.axvline(x=-860, color='k', linestyle='--', label='-860 HU') \n",
    "        plt.title(\"Prediction Histogram\", fontsize=12)\n",
    "\n",
    "        plt.subplot(2, 3, 6)\n",
    "        plt.text(0.5, 0.5, f\"Overall True Air Trapping: {true_airtrapping_overall:.2f}%\\n\"\n",
    "                           f\"Overall Predicted Air Trapping: {predicted_airtrapping_overall:.2f}%\",\n",
    "                        ha='center', va='center', fontsize=20)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Construct the filename\n",
    "        filename = f\"Batch_{current_batch_idx}_Slice_{worst_slice_idx}.png\"\n",
    "        save_path = os.path.join(save_dir, filename)\n",
    "    \n",
    "        # Save the figure\n",
    "        plt.savefig(save_path)\n",
    "    \n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, mse in enumerate(mse_values[:5]):\n",
    "    print(f\"MSE at position {idx}:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, airtrapping in enumerate(airtrapping_percentages_fake[:5]):\n",
    "    print(f\"Airtrapping percentage for Prediciton {idx}: {airtrapping}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, airtrapping in enumerate(airtrapping_percentages_real[:5]):\n",
    "    print(f\"Airtrapping percentage for Subtraction {idx}: {airtrapping}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, dicescore in enumerate(dice_scores[:5]):\n",
    "    print(f\"Airtrapping percentage for Subtraction {idx}: {dicescore}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_metrics = pd.DataFrame(columns=[\"Predicted_Image\", \"MSE_Dice\", \"Air-trp 1 [%]\", \"Air-trp 2 [%]\", \"Dice Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add values to metric dataframe\n",
    "for i, (mse, airtrapping, airtrapping_check, dice) in enumerate(zip(mse_values, airtrapping_percentages_fake, airtrapping_percentages_real, dice_scores)):\n",
    "    df_metrics.loc[i] = [i, mse, airtrapping_check, airtrapping, dice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_metrics[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save == 1:\n",
    "    try:\n",
    "        os.mkdir(f\"/data-synology/tkeller/Outputs/3D_nsteps{number_of_steps_total}_batch{batch_size}_DiceLoss_{spec}/test/\")\n",
    "        os.mkdir(f\"/data-synology/tkeller/Outputs/3D_nsteps{number_of_steps_total}_batch{batch_size}_DiceLoss_{spec}/test/metrics/\")\n",
    "        os.mkdir(f\"/data-synology/tkeller/Outputs/3D_nsteps{number_of_steps_total}_batch{batch_size}_DiceLoss_{spec}/test/plots/\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "if save == 1:\n",
    "    df_metrics.to_csv(f'/data-synology/tkeller/Outputs/3D_nsteps{number_of_steps_total}_batch{batch_size}_DiceLoss_{spec}/test/metrics/metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot metrics of df\n",
    "mean_mse = df_metrics['MSE_Dice'].mean()\n",
    "std_mse = df_metrics['MSE_Dice'].std()\n",
    "\n",
    "mean_dice = df_metrics['Dice Score'].mean()\n",
    "std_dice = df_metrics['Dice Score'].std()\n",
    "\n",
    "# Represent in the format \"mean ± SD\"\n",
    "mse_str = f\"{mean_mse:.2e} ± {std_mse:.2e}\"\n",
    "dice_str = f\"{mean_dice:.2f} ± {std_dice:.2f}\"\n",
    "\n",
    "print(f'MSE: {mse_str}')\n",
    "print(f'Dice Score: {dice_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Set the style and size\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Scatter plot\n",
    "sns.scatterplot(x=df_metrics[\"Air-trp 1 [%]\"], y=df_metrics[\"Air-trp 2 [%]\"], alpha=0.6)\n",
    "\n",
    "# Title and labels\n",
    "plt.title(\"Correlation between Air-trapping 1 (True) and Air-trapping 2 (Prediction)\", weight='bold')\n",
    "plt.xlabel(\"Air-trapping 1 [%] (True)\")\n",
    "plt.ylabel(\"Air-trapping 2 [%] (Prediction)\")\n",
    "\n",
    "# Adding the identity line\n",
    "limits = [min(plt.xlim()[0], plt.ylim()[0]), max(plt.xlim()[1], plt.ylim()[1])]\n",
    "plt.plot(limits, limits, 'r-', label='Identity Line')\n",
    "plt.legend()\n",
    "if save == 1:\n",
    "    plt.savefig(f\"/data-synology/tkeller/Outputs/3D_nsteps{number_of_steps_total}_batch{batch_size}_DiceLoss_{spec}/test/plots/Correlation_nsteps{number_of_steps_total}_batch{batch_size}{filename}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Bland-Altman plot, also known as a difference plot, is used to visualize the agreement\n",
    "# between two methods or two measurements. It plots the difference between the two measures\n",
    "# against their average.\n",
    "\n",
    "# Bland-Altman data\n",
    "average = (df_metrics[\"Air-trp 1 [%]\"] + df_metrics[\"Air-trp 2 [%]\"]) / 2\n",
    "difference = df_metrics[\"Air-trp 1 [%]\"] - df_metrics[\"Air-trp 2 [%]\"]\n",
    "\n",
    "mean_diff = difference.mean()\n",
    "std_diff = difference.std()\n",
    "\n",
    "# Set the style and size\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Scatter plot\n",
    "sns.scatterplot(x=average, y=difference, alpha=0.6)\n",
    "\n",
    "# Add mean and limits of agreement lines\n",
    "plt.axhline(mean_diff, color='red', linestyle='--', label=f'Mean diff: {mean_diff: .2f}')\n",
    "plt.axhline(mean_diff + 1.96*std_diff, color='blue', linestyle='--', label='Mean diff + 1.96*SD')  #95%\n",
    "plt.axhline(mean_diff - 1.96*std_diff, color='blue', linestyle='--', label='Mean diff - 1.96*SD')\n",
    "\n",
    "# Adding mean values next to the lines using plt.text\n",
    "x_position = max(average)\n",
    "plt.text(x_position, mean_diff + 1.96*std_diff, f'+1.96 SD: {mean_diff + 1.96*std_diff:.2f}', verticalalignment='bottom', horizontalalignment='right', color='blue')\n",
    "plt.text(x_position, mean_diff - 1.96*std_diff, f'-1.96 SD: {mean_diff - 1.96*std_diff:.2f}', verticalalignment='bottom', horizontalalignment='right', color='blue')\n",
    "\n",
    "# Title and labels\n",
    "plt.title(\"Bland-Altman Plot between Air-trapping 1 (True) and Air-trapping 2 (Prediction)\", weight='bold')\n",
    "plt.xlabel(\"Average of Air-trapping 1 [%] (True) and Air-trapping 2 [%] (Prediction)\")\n",
    "plt.ylabel(\"Difference between Air-trapping 1 [%] (True) and Air-trapping 2 [%] (Prediction)\")\n",
    "plt.legend()\n",
    "\n",
    "if save == 1:\n",
    "    plt.savefig(f\"/data-synology/tkeller/Outputs/3D_nsteps{number_of_steps_total}_batch{batch_size}_DiceLoss/test/plots/Bland_Altman_nsteps{number_of_steps_total}_batch{batch_size}{filename}.png\")\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICC\n",
    "import pingouin as pg\n",
    "\n",
    "# Reshape dataframe for ICC calculation\n",
    "df_melted = df_metrics.melt(id_vars=['Predicted_Image'], value_vars=['Air-trp 1 [%]', 'Air-trp 2 [%]'], \n",
    "                    var_name='Method', value_name='Measurement')\n",
    "\n",
    "# Calculate ICC\n",
    "icc = pg.intraclass_corr(data=df_melted, targets='Predicted_Image', raters='Method', ratings='Measurement').round(3)\n",
    "\n",
    "print(icc)\n",
    "\n",
    "# Print the ICC value for ICC1\n",
    "print(icc[icc['Type'] == 'ICC1']['ICC'].values[0])\n",
    "\n",
    "icc_val = float(f\"{icc[icc['Type'] == 'ICC1']['ICC'].values[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table of final statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'MSE: {mse_str}')\n",
    "print(f'ICC: {icc_val}')\n",
    "print(f'Dice Score: {dice_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['MSE_Dice', 'ICC', 'Dice Score'],\n",
    "    'Value': [mse_str, icc_val, dice_str]\n",
    "})\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter worst cases\n",
    "df_sorted_asce = df_metrics.sort_values(by='Dice Score', ascending=True)\n",
    "print(df_sorted_asce[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array with worst 20 cases wioth id\n",
    "predicted_image_ids = np.array(df_sorted_asce['Predicted_Image'][:50])\n",
    "\n",
    "# Now predicted_image_ids contains the IDs of interest\n",
    "print(predicted_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save == 1:\n",
    "    results_df.to_csv(f'/data-synology/tkeller/Outputs/3D_nsteps{number_of_steps_total}_batch{batch_size}_DiceLoss_{spec}/test/model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save only disease map\n",
    "save2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save2 == 1:\n",
    "    try:\n",
    "        os.mkdir(f'/data-synology/tkeller/Outputs/3D_nsteps{number_of_steps_total}_batch{batch_size}_DiceLoss_{spec}/DiseaseMap/')\n",
    "    except:\n",
    "        print('Couldnt create directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator3 = data_generator2(test_dataset_paths, test_mask_insp_paths, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "desired_indices = [1,2,3,4,5]  # Specify the indices of the images you want\n",
    "count = 0  # Initialize the counter\n",
    "\n",
    "# Iterate through the generator\n",
    "for batch_input, batch_output, batch_masks in test_generator3:\n",
    "    #print(f'Batch {count} processed')\n",
    "    for element in desired_indices:\n",
    "        int_element = np.floor(element).astype(int)\n",
    "        print(f'Processing index {int_element}')\n",
    "        \n",
    "        predictions, mask2 = model(batch_input, training=False)\n",
    "        masked_predictions = []\n",
    "\n",
    "        for prediction, mask in zip(predictions, batch_masks):\n",
    "            mask_pred = prediction * mask\n",
    "            masked_predictions.append(mask_pred)\n",
    "\n",
    "        # Process and plot the first image in each batch (or adjust as needed)\n",
    "        j = 0  # first image of each batch\n",
    "    \n",
    "        # Select images for creating disease map\n",
    "        insp_ct_ds = (batch_input[j] * 3000)[96] # masked_inspiratory[j] * 3000\n",
    "        exp_ct_deform_ds = (batch_output[j] - batch_input[j])[96] # masked_subtraction[j]\n",
    "        prediction_sub = (masked_predictions[j] - batch_input[j])[96]# masked_predictions[j]\n",
    "        insp_whole = (batch_masks[j])[96] # np.where((test_mask_images[j] > 0) & (test_mask_images[j] < 6), 1, 0)\n",
    "        \n",
    "        # Remove the singular third dimension\n",
    "        insp_ct_ds_2d = np.rot90(np.squeeze(insp_ct_ds), k=-1)\n",
    "        exp_ct_deform_ds_2d = np.rot90(np.squeeze(exp_ct_deform_ds), k=-1)\n",
    "        sub_ct_deform_pred_2d = np.rot90(np.squeeze(prediction_sub), k=-1)\n",
    "        insp_whole_2d = np.rot90(np.squeeze(insp_whole), k=-1)\n",
    "        \n",
    "        # Calculate difference\n",
    "        insp_slc_mask_smooth = gaussian_filter(insp_ct_ds_2d, sigma=1) * insp_whole_2d\n",
    "        diff_slc_mask_smooth = np.abs(gaussian_filter(exp_ct_deform_ds_2d * 3000, sigma=1)) * insp_whole_2d\n",
    "        diff_slc_pred_mask_smooth = np.abs(gaussian_filter(sub_ct_deform_pred_2d * 3000, sigma=1)) * insp_whole_2d\n",
    "        \n",
    "        # Flip images\n",
    "        insp_slc = np.flipud(np.fliplr(insp_ct_ds_2d))\n",
    "        insp_mask = np.flipud(np.fliplr(insp_whole_2d))\n",
    "        insp_slc_mask = np.flipud(np.fliplr(insp_slc_mask_smooth))\n",
    "        diff_slc_pred_mask = np.flipud(np.fliplr(diff_slc_pred_mask_smooth))  # prediction\n",
    "        diff_slc_mask = np.flipud(np.fliplr(diff_slc_mask_smooth))  # ground truth\n",
    "\n",
    "        # Red parts with emphysema\n",
    "        insp_map = cm.get_cmap('Reds', 512)(np.linspace(1,0,512))\n",
    "        insp_map[:,3] = np.linspace(1,0,512)\n",
    "        insp_map[:,0:3] = insp_map[256,0:3]\n",
    "        insp_map=ListedColormap(insp_map)\n",
    "        \n",
    "        # Blue parts with air-trapping\n",
    "        diff_map = cm.get_cmap('Blues', 512)(np.linspace(1,0,512))\n",
    "        diff_map[:,3] = np.linspace(1,0,512)\n",
    "        diff_map[:,0:3] = diff_map[128,0:3]\n",
    "        diff_map=ListedColormap(diff_map)\n",
    "        \n",
    "        # Create the figure\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        \n",
    "        # Display the first image on the left side\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(insp_slc, cmap='gray', vmin=-400-1500, vmax=-400+1500, origin='lower')\n",
    "        plt.imshow(np.ma.masked_where(insp_mask == 0, diff_slc_mask), cmap=diff_map, vmin=0, vmax=100, alpha=0.5)\n",
    "        plt.imshow(np.ma.masked_where(insp_mask == 0, insp_slc_mask), cmap=insp_map, vmin=-1000, vmax=-925, alpha=0.5)\n",
    "        plt.title(f\"Ground-Truth Index {int_element}\", fontweight='bold', fontsize=20)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Display the second image on the right side\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(insp_slc, cmap='gray', vmin=-400-1500, vmax=-400+1500, origin='lower')\n",
    "        plt.imshow(np.ma.masked_where(insp_mask == 0, diff_slc_pred_mask), cmap=diff_map, vmin=0, vmax=100, alpha=0.5)\n",
    "        plt.imshow(np.ma.masked_where(insp_mask == 0, insp_slc_mask), cmap=insp_map, vmin=-1000, vmax=-925, alpha=0.5)\n",
    "        plt.title(f\"Prediction Index {int_element}\", fontweight='bold', fontsize=20)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save2 == 1:\n",
    "            plt.savefig(f'/data-synology/tkeller/Outputs/3D_nsteps{number_of_steps_total}_batch{batch_size}_DiceLoss_{spec}/DiseaseMap/DiseaseMap_defEx_{count}.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        #print(int_element\n",
    "\n",
    "    # Increment the counter after each batch\n",
    "    count += 1 \n",
    "\n",
    "    if count > len(desired_indices):\n",
    "        break  # Exit loop after the highest index is surpassed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nibabel as nib\n",
    "# aff = np.array([[0,-1,0,0],[-1,0,0,0],[0,0,1,0],[0,0,0,1]])\n",
    "# img = nib.Nifti1Image(np.float32(img), aff)\n",
    "# nib.save(img, 'filename.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator3 = data_generator(test_dataset_paths, test_mask_insp_paths, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a single batch from the generator\n",
    "try:\n",
    "    batch_input, batch_output, batch_masks = next(test_generator3)\n",
    "    # Generate predictions for this batch\n",
    "    print(f\"Batch shapes -- Input: {batch_input.shape}, Output: {batch_output.shape}, Masks: {batch_masks.shape}, Predictions: {predictions.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting batch from generator or generating predictions: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a single 3D image as a NIfTI file\n",
    "def save_nifti_with_affine(batch_input, batch_output, predictions, mask, index, output_dir):\n",
    "    \"\"\"\n",
    "    Saves 3D images with a specific affine transformation as NIfTI files.\n",
    "\n",
    "    Parameters:\n",
    "    - batch_input: 4D numpy array of input images [batch_size, depth, height, width].\n",
    "    - batch_output: 4D numpy array of output images [batch_size, depth, height, width].\n",
    "    - predictions: 4D numpy array of model predictions [batch_size, depth, height, width].\n",
    "    - mask: 4D numpy array of masks [batch_size, depth, height, width].\n",
    "    - index: int, index of the image in the batch to save.\n",
    "    - output_dir: str, directory to save the NIfTI files.\n",
    "    \"\"\"\n",
    "    # Affine matrix for flipping x and y axes\n",
    "    affine = np.array([[0, -1, 0, 0],\n",
    "                       [-1, 0, 0, 0],\n",
    "                       [0, 0, 1, 0],\n",
    "                       [0, 0, 0, 1]])\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Function to save a single 3D image as a NIfTI file\n",
    "    def save_image(data, filename):\n",
    "        # Convert data to float32 as recommended\n",
    "        data_float32 = np.float32(data)\n",
    "        # Create and save the NIfTI image\n",
    "        img = nib.Nifti1Image(data_float32, affine)\n",
    "        nib.save(img, filename)\n",
    "\n",
    "    # Save each image\n",
    "    save_image(batch_input[index], os.path.join(output_dir, f'input_{index}.nii'))\n",
    "    save_image(batch_output[index] - batch_input[index], os.path.join(output_dir, f'output_deformation_{index}.nii'))\n",
    "    save_image(predictions[index] - batch_input[index], os.path.join(output_dir, f'prediction_deformation_{index}.nii'))\n",
    "    save_image(mask[index], os.path.join(output_dir, f'mask_{index}.nii'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "# Specify the index of the image you want to save, 0 for the first image\n",
    "batch_index = 0\n",
    "\n",
    "# Specify your output directory here, ensure it exists or you have permissions to write to it\n",
    "output_dir = f'/data-synology/tkeller/Outputs/3D_nsteps{number_of_steps_total}_batch{batch_size}_DiceLoss/DiseaseMap/'\n",
    "\n",
    "# Now call the function with the first image data\n",
    "save_nifti_with_affine(batch_input, batch_output, predictions, batch_masks, batch_index, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN1CR7+GzcjQFmlCHH6fZ8Y",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
