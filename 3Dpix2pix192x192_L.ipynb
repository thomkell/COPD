{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlpGhzv41Qmh"
   },
   "source": [
    "# load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select test_training(1) or training(0)\n",
    "test = 1\n",
    "\n",
    "# paramters training\n",
    "nbatch = 1\n",
    "nepochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/tkeller/anaconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "\u001b[0m\u001b[01;34m2D_192x192px_middleSlice\u001b[0m/        \u001b[01;34mbackupFiles\u001b[0m/\n",
      "2Dpix2pix192x192_L.ipynb         ImageGenerator_2D_192x192_L.ipynb\n",
      "2Dpix2pix192x192_L_Load.ipynb    metricsPlot.ipynb\n",
      "2Dpix2pix192x192_L_NoMask.ipynb  \u001b[01;34moldFiles\u001b[0m/\n",
      "3Dpix2pix192x192_L.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data-synology/anlee\n"
     ]
    }
   ],
   "source": [
    "cd /data-synology/anlee/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 435,
     "status": "ok",
     "timestamp": 1691536112346,
     "user": {
      "displayName": "Thomas Keller",
      "userId": "17223430032597201940"
     },
     "user_tz": 420
    },
    "id": "vgh3OIul1RvA",
    "outputId": "e2130622-09d6-4496-d403-59e9c73ad2ae",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/tkeller/anaconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      " \u001b[0m\u001b[01;34mCOPDGene\u001b[0m/            \u001b[01;32mfilenamestrain.npy\u001b[0m*   \u001b[01;34msoftmasks\u001b[0m/           \u001b[01;34m'testing ccc'\u001b[0m/\n",
      " \u001b[01;34mfeatures\u001b[0m/            \u001b[01;32mfilenamesval.npy\u001b[0m*     \u001b[01;34msoftmasks_summer23\u001b[0m/   \u001b[01;32mtestnpy.npy\u001b[0m*\n",
      " \u001b[01;34mfeatures_CCC\u001b[0m/        \u001b[01;34mheatmaps\u001b[0m/             \u001b[01;32mtest\u001b[0m*\n",
      " \u001b[01;34mfeatures_CCC_meas\u001b[0m/   \u001b[01;34msaliency\u001b[0m/             \u001b[01;32mtest2\u001b[0m*\n",
      " \u001b[01;32mfilenamestest.npy\u001b[0m*   \u001b[01;34msmoothgrad\u001b[0m/           \u001b[01;32mtest3\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4870,
     "status": "ok",
     "timestamp": 1691536123960,
     "user": {
      "displayName": "Thomas Keller",
      "userId": "17223430032597201940"
     },
     "user_tz": 420
    },
    "id": "5_BvSxkr3BBw",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-22 14:04:43.231684: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-22 14:04:43.781735: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mndi\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnilearn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnil\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlist_physical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# random seeds\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/__init__.py:438\u001b[0m\n\u001b[1;32m    436\u001b[0m _kernel_dir \u001b[38;5;241m=\u001b[39m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_tf_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(_kernel_dir):\n\u001b[0;32m--> 438\u001b[0m   \u001b[43m_ll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_kernel_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Load third party dynamic kernels.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _s \u001b[38;5;129;01min\u001b[39;00m _site_packages_dirs:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/framework/load_library.py:151\u001b[0m, in \u001b[0;36mload_library\u001b[0;34m(library_location)\u001b[0m\n\u001b[1;32m    148\u001b[0m     kernel_libraries \u001b[38;5;241m=\u001b[39m [library_location]\n\u001b[1;32m    150\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m lib \u001b[38;5;129;01min\u001b[39;00m kernel_libraries:\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mpy_tf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_LoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m       errno\u001b[38;5;241m.\u001b[39mENOENT,\n\u001b[1;32m    156\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe file or folder to load kernel libraries from does not exist.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    157\u001b[0m       library_location)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# includes&imports\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #for CPU \"\"\n",
    "\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndi\n",
    "import nilearn as nil\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# random seeds\n",
    "np.random.seed(16)\n",
    "tf.random.set_seed(16)\n",
    "tf.keras.utils.set_random_seed(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mhg2AoOLcAf"
   },
   "source": [
    "Build an input pipeline with image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31949,
     "status": "ok",
     "timestamp": 1689621403431,
     "user": {
      "displayName": "Thomas Keller",
      "userId": "17223430032597201940"
     },
     "user_tz": 420
    },
    "id": "Szib1HgsLasf",
    "outputId": "35cd0da8-264b-4d44-b2bc-696a97668cb6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get image pathes for input and target images\n",
    "from tqdm import tqdm\n",
    "\n",
    "root_directory = 'COPDGene/'  # Replace with the actual path to your root directory\n",
    "inputImageName = ('insp_ct_ds.nii') #('insp_ct.nii')\n",
    "outputImageName = ('exp_ct_deform_ds.nii')  # Extensions of the target image files\n",
    "# maskImageName = ('insp_mask_cat.nii')  # Mask image\n",
    "\n",
    "\n",
    "def search_images(directory, image_list, name):\n",
    "  for root, dirs, files in tqdm(os.walk(directory)):\n",
    "        for file in files:\n",
    "            if file.endswith(name):\n",
    "                image_path = os.path.join(root, file)\n",
    "                image_list.append(image_path.replace('\\0', ''))  # Add the image path to the list, replace termination character\n",
    "\n",
    "# Create an empty list to store image paths\n",
    "inputImagePath = []\n",
    "outputImagePath = []\n",
    "\n",
    "\n",
    "# Call the search_image function with the root directory\n",
    "search_images(root_directory, inputImagePath, inputImageName)\n",
    "search_images(root_directory, outputImagePath, outputImageName)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# check if path loaded\n",
    "if inputImagePath:\n",
    "    print(\"Image input paths:\")\n",
    "    for path in inputImagePath[:5]:\n",
    "        print(path)\n",
    "else:\n",
    "    print(\"No image files found in the directory tree.\")\n",
    "\n",
    "if outputImagePath:\n",
    "    print(\"Image ouput paths:\")\n",
    "    for path in outputImagePath[:5]:\n",
    "        print(path)\n",
    "else:\n",
    "    print(\"No image files found in the directory tree.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1689621403984,
     "user": {
      "displayName": "Thomas Keller",
      "userId": "17223430032597201940"
     },
     "user_tz": 420
    },
    "id": "HiHMHEnb-r12",
    "outputId": "f7eb6796-8c1c-4eac-d482-3f5d286d959d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert image paths to lists\n",
    "inputImagePath = list(inputImagePath)\n",
    "outputImagePath = list(outputImagePath)\n",
    "\n",
    "# Split the data into training and test sets list\n",
    "train_input, test_input, train_output, test_output = train_test_split(\n",
    "    inputImagePath, outputImagePath, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the data into test and validation sets list\n",
    "val_input, test_input, val_output, test_output = train_test_split(\n",
    "    test_input, test_output, test_size=0.5, random_state=42)\n",
    "\n",
    "print('training data: ' + str(len(train_input)))\n",
    "print('validation data: ' + str(len(val_input)))\n",
    "print('test data: ' + str(len(test_input)))\n",
    "\n",
    "if train_input:\n",
    "    print(\"Image input paths:\")\n",
    "    for path in train_input[:5]:\n",
    "        print(path)\n",
    "else:\n",
    "    print(\"No image files found in the directory tree.\")\n",
    "\n",
    "if train_output:\n",
    "    print(\"Image ouput paths:\")\n",
    "    for path in train_output[:5]:\n",
    "        print(path)\n",
    "else:\n",
    "    print(\"No image files found in the directory tree.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_middle_directory(file_path):\n",
    "    parts = file_path.split('/')\n",
    "    if len(parts) >= 3:\n",
    "        return parts[1]\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print((test_input, test_output))\n",
    "import csv\n",
    "\n",
    "data_pairs = (test_input, test_output)\n",
    "\n",
    "try:\n",
    "    middle_directories = [extract_middle_directory(path) for path in test_input]\n",
    "    #print(middle_directories)\n",
    "\n",
    "    with open(f'/data-synology/tkeller/Outputs/testDataDirName_{nepochs}_{nbatch}.csv', \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        for input_data in middle_directories:\n",
    "            writer.writerow([input_data])\n",
    "    print('Data saved to', f'/data-synology/tkeller/Outputs/testDataDirName_{nepochs}_{nbatch}.csv')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1689621404457,
     "user": {
      "displayName": "Thomas Keller",
      "userId": "17223430032597201940"
     },
     "user_tz": 420
    },
    "id": "egjy5BzVO7C9",
    "outputId": "b284ef91-cafe-4272-a7de-815431919f99",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_input[0])\n",
    "image_dataX = nib.load(train_input[0])\n",
    "image = image_dataX.get_fdata()\n",
    "\n",
    "#i,j,k = np.array(image.shape) // 2  # Assuming you want the middle slice along the first axis\n",
    "# i = 95\n",
    "#plt.imshow(image[i, :, :].T, cmap='Greys_r', origin='lower')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.shape) # prints the shape of the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1689621404458,
     "user": {
      "displayName": "Thomas Keller",
      "userId": "17223430032597201940"
     },
     "user_tz": 420
    },
    "id": "_oGRiQDMJVyG",
    "outputId": "e5db1238-9513-464a-98a8-3da7e28704a9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_data = np.array(image)\n",
    "\n",
    "pixel_min = image_data.min()\n",
    "pixel_max = image_data.max()\n",
    "\n",
    "# Print the pixel range\n",
    "print(\"Pixel range: {} - {}\".format(pixel_min, pixel_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1689621404636,
     "user": {
      "displayName": "Thomas Keller",
      "userId": "17223430032597201940"
     },
     "user_tz": 420
    },
    "id": "1y90o2lWHnJI",
    "outputId": "c452bcdc-618b-41b6-a04e-d9c4787ba729",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image function to load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Trc3urJsKrYI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(file_path):\n",
    "    # print path to see which image is loaded\n",
    "    print(file_path)\n",
    "    \n",
    "    # load nibable image\n",
    "    image_data = nib.load(file_path).get_fdata()\n",
    "    \n",
    "    # find middle slice\n",
    "    # i, j, k = np.array(image_data.shape) // 2\n",
    "\n",
    "    # image = image_data[i, :, :].T\n",
    "\n",
    "    # normalize\n",
    "    pixel_min = image_data.min()\n",
    "    pixel_max = image_data.max()\n",
    "    image = ((image_data - pixel_min) / (pixel_max - pixel_min)) * 2 - 1\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeKgX6A9hwAM"
   },
   "source": [
    "# 2D inspiratory to deformed expiratory images pix2pix from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgOd0VWRnUIi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/data-synology/tkeller/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePYwhYaKoPaE"
   },
   "source": [
    "# Implement the PatchGAN Discriminator Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 151,
     "status": "ok",
     "timestamp": 1689621404786,
     "user": {
      "displayName": "Thomas Keller",
      "userId": "17223430032597201940"
     },
     "user_tz": 420
    },
    "id": "NzoAC3WZpCag",
    "outputId": "b8219033-aebd-4073-ef21-10be7e6c37d9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2305,
     "status": "ok",
     "timestamp": 1689621407089,
     "user": {
      "displayName": "Thomas Keller",
      "userId": "17223430032597201940"
     },
     "user_tz": 420
    },
    "id": "b9zF0t_7lfe9",
    "outputId": "f06a07ce-0b9e-4b23-c9fc-3ce697271bad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example of defining a 70x70 patchgan discriminator model\n",
    "#from keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from keras.layers import Conv3D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import BatchNormalization, GaussianNoise\n",
    "#from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "# define the discriminator model\n",
    "def define_discriminator(image_shape):\n",
    " # weight initialization\n",
    " init = RandomNormal(stddev=0.02, seed=None)\n",
    " # source image input\n",
    " in_src_image = Input(shape=image_shape)\n",
    " # target image input\n",
    " in_target_image = Input(shape=image_shape)\n",
    "\n",
    " # Add Gaussian noise\n",
    " # Add Gaussian Noise layer to the src image input\n",
    " # noisy_src_image = GaussianNoise(0.1)(in_src_image)  # Adjust the noise level as needed\n",
    " # Add Gaussian Noise layer to the target image input\n",
    " # noisy_target_image = GaussianNoise(0.1)(in_target_image)  # Adjust the noise level as needed\n",
    "\n",
    " # concatenate images channel-wise\n",
    " # merged = Concatenate()([noisy_src_image, noisy_target_image])\n",
    " merged = Concatenate()([in_src_image, in_target_image])\n",
    "    \n",
    " # C64\n",
    " d = Conv3D(64, (4,4,4), strides=(2,2,2), padding='same', kernel_initializer=init)(merged)\n",
    " d = LeakyReLU(alpha=0.2)(d)\n",
    " # C128\n",
    " d = Conv3D(128, (4,4,4), strides=(2,2,2), padding='same', kernel_initializer=init)(d)\n",
    " d = BatchNormalization()(d)\n",
    " d = LeakyReLU(alpha=0.2)(d)\n",
    " # C256\n",
    " d = Conv3D(256, (4,4,4), strides=(2,2,2), padding='same', kernel_initializer=init)(d)\n",
    " d = BatchNormalization()(d)\n",
    " d = LeakyReLU(alpha=0.2)(d)\n",
    " # C512\n",
    " d = Conv3D(512, (4,4,4), strides=(2,2,2), padding='same', kernel_initializer=init)(d)\n",
    " d = BatchNormalization()(d)\n",
    " d = LeakyReLU(alpha=0.2)(d)\n",
    " # second last output layer\n",
    " d = Conv3D(512, (4,4,4), padding='same', kernel_initializer=init)(d)\n",
    " d = BatchNormalization()(d)\n",
    " d = LeakyReLU(alpha=0.2)(d)\n",
    " # patch output\n",
    " d = Conv3D(1, (4,4,4), padding='same', kernel_initializer=init)(d)\n",
    " patch_out = Activation('sigmoid')(d)\n",
    " # define model\n",
    " model = Model([in_src_image, in_target_image], patch_out)\n",
    " # compile model\n",
    " opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    " model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5], metrics=['accuracy'])\n",
    " return model\n",
    "\n",
    "# define image shape\n",
    "image_shape = (192,192,192,1)\n",
    "# create the model\n",
    "model = define_discriminator(image_shape)\n",
    "\n",
    "# summarize the model\n",
    "model.summary()\n",
    "# plot the model\n",
    "# plot_model(model, to_file='/data-synology/tkeller/Outputs/discriminator_model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8BXEcWvoz53"
   },
   "source": [
    "# Implement the U-Net Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1658,
     "status": "ok",
     "timestamp": 1689621408744,
     "user": {
      "displayName": "Thomas Keller",
      "userId": "17223430032597201940"
     },
     "user_tz": 420
    },
    "id": "V2_vIFLF9X_6",
    "outputId": "7f97adf2-2cef-49a4-85f7-30899c6a8808",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example of defining a u-net encoder-decoder generator model\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from keras.layers import Conv3D\n",
    "from keras.layers import Conv3DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "#from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.layers import UpSampling3D\n",
    "\n",
    "# define an encoder block\n",
    "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# add downsampling layer\n",
    "\tg = Conv3D(n_filters, (4,4,4), strides=(2,2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "\t# conditionally add batch normalization\n",
    "\tif batchnorm:\n",
    "\t\tg = BatchNormalization()(g, training=True)\n",
    "\t# leaky relu activation\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\treturn g\n",
    "\n",
    "# define a decoder block\n",
    "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# add upsampling layer\n",
    "\tg = Conv3DTranspose(n_filters, (4,4,4), strides=(2,2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "\t#g = UpSampling3D(size=(2,2,2))(layer_in)\n",
    "\t#g = Conv3D(n_filters,(3,3,3),strides = (1,1,1), padding='same', kernel_initializer=init)(g)\n",
    "    # add batch normalization\n",
    "\tg = BatchNormalization()(g, training=True)\n",
    "\t# conditionally add dropout\n",
    "\tif dropout:\n",
    "\t\tg = Dropout(0.5)(g, training=True)\n",
    "\t# merge with skip connection\n",
    "\tg = Concatenate()([g, skip_in])\n",
    "\t# relu activation\n",
    "\tg = Activation('relu')(g)\n",
    "\treturn g\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(image_shape=(192,192,192,1)): #image_shape=(256,256,1)\n",
    "  # weight initialization\n",
    "  init = RandomNormal(stddev=0.02)\n",
    "  # image input\n",
    "  in_image = Input(shape=image_shape)\n",
    "  # encoder model: C64-C128-C256-C512-C512-C512\n",
    "  e1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
    "  e2 = define_encoder_block(e1, 128)\n",
    "  e3 = define_encoder_block(e2, 256)\n",
    "  e4 = define_encoder_block(e3, 512)\n",
    "  e5 = define_encoder_block(e4, 512)\n",
    "  e6 = define_encoder_block(e5, 512)\n",
    "  b = Conv3D(512, (3,3,3), strides=(1,1,1), padding='same', kernel_initializer=init)(e6)\n",
    "  b = Activation('relu')(b)\n",
    "  d1 = Conv3D(512, (3,3,3), strides=(1,1,1), padding='same', kernel_initializer=init)(b)\n",
    "  d1 = Activation('relu')(d1)\n",
    "  # decoder model: CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n",
    "  d2 = decoder_block(d1, e5, 512)\n",
    "  d3 = decoder_block(d2, e4, 512)\n",
    "  d4 = decoder_block(d3, e3, 512, dropout=False)\n",
    "  d5 = decoder_block(d4, e2, 256, dropout=False)\n",
    "  d6 = decoder_block(d5, e1, 128, dropout=False)\n",
    "  # output\n",
    "  # g = Conv2DTranspose(1, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d6)\n",
    "  g = UpSampling3D(size=(2,2,2))(d6)\n",
    "  g = Conv3D(1,(3,3,3),strides = (1,1,1), padding='same', kernel_initializer=init)(g)\n",
    "  out_image = Activation('tanh')(g)\n",
    "\n",
    "  # define model\n",
    "  print(in_image)\n",
    "  model = Model(in_image, out_image)\n",
    "  return model\n",
    "\n",
    "# define image shape\n",
    "image_shape = (192,192,192,1)\n",
    "# create the model\n",
    "model = define_generator(image_shape)\n",
    "# summarize the model\n",
    "model.summary()\n",
    "# plot the model\n",
    "# plot_model(model, to_file='/data-synology/tkeller/generator_model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-MpV9G9rlBr"
   },
   "source": [
    "# Implement Adversarial and L1 Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMNt5tg8qU-p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model, image_shape):\n",
    "  # make weights in the discriminator not trainable\n",
    "  for layer in d_model.layers:\n",
    "    if not isinstance(layer, BatchNormalization):\n",
    "      layer.trainable = False\n",
    "      # define the source image\n",
    "      in_src = Input(shape=image_shape)\n",
    "      # connect the source image to the generator input\n",
    "      gen_out = g_model(in_src)\n",
    "      # connect the source input and generator output to the discriminator input\n",
    "      dis_out = d_model([in_src, gen_out])\n",
    "      # src image as input, generated image and classification output\n",
    "      model = Model(in_src, [dis_out, gen_out])\n",
    "      # compile model\n",
    "      opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "      model.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
    "      return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "executionInfo": {
     "elapsed": 610,
     "status": "ok",
     "timestamp": 1689621409353,
     "user": {
      "displayName": "Thomas Keller",
      "userId": "17223430032597201940"
     },
     "user_tz": 420
    },
    "id": "hzXUYF1WsSTk",
    "outputId": "6ed8eb13-d06c-455f-b423-4bc14f3a2d78",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define image shape\n",
    "image_shape = (192,192,192,1)\n",
    "# define the models\n",
    "d_model = define_discriminator(image_shape)\n",
    "g_model = define_generator(image_shape)\n",
    "# define the composite model\n",
    "gan_model = define_gan(g_model, d_model, image_shape)\n",
    "# summarize the model\n",
    "gan_model.summary()\n",
    "# plot the model\n",
    "plot_model(gan_model, to_file='/data-synology/tkeller/gan_model_plot.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLSFLZEjtldP"
   },
   "source": [
    "# Update Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZDItWmaslvQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from random import randint\n",
    "from random import sample\n",
    "\n",
    "# select a batch of random samples, returns images and target\n",
    "def generate_real_samples(paths, n_samples, patch_shape, iterationCount):\n",
    "\n",
    "    # random_indices = np.random.choice(range(trainA.shape[0]), size = n_samples, replace = False)\n",
    "    train_input_paths, train_output_paths = paths\n",
    "\n",
    "    # create array with random indices\n",
    "    random_indices = sample(range(len(train_input_paths)), n_samples)\n",
    "    print('image batch: ' + str(random_indices))\n",
    "\n",
    "    # load random indices paths\n",
    "    random_input_paths = [train_input[idx] for idx in random_indices]\n",
    "    random_output_paths = [train_output[idx] for idx in random_indices]\n",
    "    \n",
    "\n",
    "    train_input_images = []\n",
    "    train_output_images = []\n",
    "\n",
    "    # for idx in random_indices:\n",
    "    for input_path, output_path in zip(random_input_paths, random_output_paths):\n",
    "        input_image = load_image(input_path)\n",
    "        output_image = load_image(output_path)\n",
    "        train_input_images.append(np.expand_dims(input_image, -1))\n",
    "        train_output_images.append(np.expand_dims(output_image, -1))\n",
    "\n",
    "    # convert to np.array\n",
    "    train_dataset = (train_input_images, train_output_images)\n",
    "    dataset = np.array(train_dataset)\n",
    "    trainA = dataset[0]\n",
    "    trainB = dataset[1]\n",
    "\n",
    "    # generate 'real' class labels (1)\n",
    "    y = np.ones((n_samples, patch_shape, patch_shape, 1))\n",
    "    return [trainA, trainB], y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zV0TT8C-tqgl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.control_flow_ops import type_spec\n",
    "\n",
    "# generate a batch of images, returns images and targets\n",
    "def generate_fake_samples(g_model, samples, patch_shape):\n",
    "    \n",
    " # generate fake instance\n",
    " X = g_model.predict(samples)\n",
    "    \n",
    " # create 'fake' class labels (0)\n",
    " y = np.zeros((len(X), patch_shape, patch_shape, 1))\n",
    "    \n",
    " return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRii5vT0tsmc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "# create a line plot of loss for the gan and save to file\n",
    "def plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist):\n",
    "\n",
    " # plot loss\n",
    " pyplot.subplot(2, 1, 1)\n",
    " pyplot.plot(d1_hist, label='d-real')\n",
    " pyplot.plot(d2_hist, label='d-fake')\n",
    " pyplot.plot(g_hist, label='gen')\n",
    " pyplot.legend()\n",
    "    \n",
    " # plot discriminator accuracy\n",
    " pyplot.subplot(2, 1, 2)\n",
    " pyplot.plot(a1_hist, label='acc-real')\n",
    " pyplot.plot(a2_hist, label='acc-fake')\n",
    " pyplot.legend()\n",
    "    \n",
    " # save plot to file\n",
    " pyplot.savefig('/data-synology/tkeller/Outputs/plot_line_plot_loss.png', dpi=1000)\n",
    " pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZYHNi3rtwEU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# train pix2pix models\n",
    "def train(d_model, g_model, gan_model, paths, n_epochs, n_batch, n_patch=12):\n",
    "\n",
    " # unpack tuple\n",
    " train_input_paths, train_output_paths = paths\n",
    " print('number of training inputs: ' + str(len(train_input_paths)))\n",
    "\n",
    " # calculate the number of batches per training epoch\n",
    " bat_per_epo = int(len(train_input_paths) / n_batch)\n",
    " print('number of batches per epoch: ' + str(bat_per_epo))\n",
    "\n",
    " # calculate the number of training iterations\n",
    " n_steps = int(bat_per_epo * n_epochs)\n",
    " print('number of steps: ' + str(n_steps))\n",
    "\n",
    " # prepare lists for storing stats each iteration\n",
    " d1_hist, d2_hist, g_hist, a1_hist, a2_hist = list(), list(), list(), list(), list()\n",
    "\n",
    " # manually enumerate epochs\n",
    " #for i in range(n_steps):\n",
    " for i in range(0, n_steps):\n",
    "  # select a batch of real samples\n",
    "  [X_realA, X_realB], y_real = generate_real_samples(paths, n_batch, n_patch, i)\n",
    "  # generate a batch of fake samples\n",
    "  X_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
    "  # update discriminator for real samples\n",
    "  d_loss1, d_acc1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
    "  # update discriminator for generated samples\n",
    "  d_loss2, d_acc2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
    "  # update the generator\n",
    "  g_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
    "  # summarize performance\n",
    "  print('>%d/%d, d1[%.3f] d2[%.3f] g[%.3f], a1=%d, a2=%d' % (i+1, n_steps, d_loss1, d_loss2, g_loss, int(100*d_acc1), int(100*d_acc2)))\n",
    "   # record history\n",
    "  d1_hist.append(d_loss1)\n",
    "  d2_hist.append(d_loss2)\n",
    "  g_hist.append(g_loss)\n",
    "  a1_hist.append(d_acc1)\n",
    "  a2_hist.append(d_acc2)\n",
    "\n",
    "  # number of steps to save model\n",
    "  if ((i % 100) == 0 and i != 0):\n",
    "    # Save the generator/discriminator model in h5\n",
    "    gan_model.save(f'/data-synology/tkeller/Outputs/modelCheckp_{n_stepsX}_{nbatch}/gan_model_{n_stepsX}_{nbatch}_{i}.h5')\n",
    "\n",
    "    # Save the generator/discriminator weights in h5\n",
    "    gan_model.save_weights(f'/data-synology/tkeller/Outputs/modelCheckp_{n_stepsX}_{nbatch}/gan_model_weights_{n_stepsX}_{nbatch}_{i}.h5')\n",
    "\n",
    "    # Save the generator/discriminator model in keras format\n",
    "    gan_model.save(f'/data-synology/tkeller/Outputs/modelCheckp_{n_stepsX}_{nbatch}/gan_model_{n_stepsX}_{nbatch}_{i}.keras')\n",
    "\n",
    "    plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist)\n",
    "\n",
    "    rows = zip(d1_hist, d2_hist, g_hist, a1_hist, a2_hist)\n",
    "\n",
    "    with open(f'/data-synology/tkeller/Outputs/metrics_{n_stepsX}_{nbatch}_{i}.csv', \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        for row in rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    shutil.move('/data-synology/tkeller/Outputs/plot_line_plot_loss.png', f'/data-synology/tkeller/Outputs/nsteps{n_stepsX}_batch{nbatch}/plot_line_plot_loss{i}.png')\n",
    "    shutil.move(f'/data-synology/tkeller/Outputs/metrics_{n_stepsX}_{nbatch}_{i}.csv', f'/data-synology/tkeller/Outputs/nsteps{n_stepsX}_batch{nbatch}/metrics_{n_stepsX}_{nbatch}_{i}.csv')\n",
    "\n",
    " plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_5OO0vGtsLp"
   },
   "source": [
    "Dataset reduction for test purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1689621409595,
     "user": {
      "displayName": "Thomas Keller",
      "userId": "17223430032597201940"
     },
     "user_tz": 420
    },
    "id": "_geQcJdvN2EL",
    "outputId": "b30605b1-f5fe-4241-b0da-718b069f1ca7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select test or train\n",
    "if test == 1:\n",
    "    train_input_smaller = train_input[0::128] # every 16th item, starting from the 1th item\n",
    "    train_output_smaller = train_output[0::128] # every 16th item starting from the 1th item\n",
    "\n",
    "    train_dataset_paths = (train_input_smaller, train_output_smaller)\n",
    "    \n",
    "else:\n",
    "    train_dataset_paths = (train_input, train_output)\n",
    "\n",
    "# calucalte steps and so on\n",
    "length = len(train_dataset_paths[0])\n",
    "print('length:')\n",
    "print(length)\n",
    "\n",
    "# batch per epoch\n",
    "bat_per_epoX = int(length / nbatch)\n",
    "print('bat_per_epoX:')\n",
    "print(bat_per_epoX)\n",
    "\n",
    "# calculate the number of training iterations\n",
    "n_stepsX = int(bat_per_epoX * nepochs)\n",
    "print('n_stepsX:')\n",
    "print(n_stepsX)\n",
    "\n",
    "# create folder for \"nsteps{n_stepsX}_batch{nbatch}\"\n",
    "#os.getcwd()\n",
    "#os.chdir(\"/data-synology/tkeller/Outputs\")\n",
    "try:\n",
    "    os.mkdir(f\"/data-synology/tkeller/Outputs/nsteps{n_stepsX}_batch{nbatch}\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPinIE8ewaHL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/data-synology/anlee/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoK_LCae1YAm"
   },
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6327128,
     "status": "error",
     "timestamp": 1689627736718,
     "user": {
      "displayName": "Thomas Keller",
      "userId": "17223430032597201940"
     },
     "user_tz": 420
    },
    "id": "yaKd1EA_ty-t",
    "outputId": "87a3e280-9ea3-45ac-e3fb-94793b0dcc03",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_dataset_paths = (train_input, train_output)\n",
    "\n",
    "# train model\n",
    "train(d_model, g_model, gan_model, train_dataset_paths, nepochs, nbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oHOFKn0LbFN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQoXKGzFLl79",
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "#os.chdir(\"/content/gdrive/My Drive\")\n",
    "\n",
    "#os.mkdir(f\"nsteps{n_stepsX}_batch{nbatch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/data-synology/tkeller/Outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j58nAUZeLoCs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92xvsw-FLA6r",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save/load model\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#d_model.save(f'/home/tkeller/Outputs/nsteps{n_stepsX}_batch{nbatch}/d_model_{n_stepsX}_{nbatch}.h5')\n",
    "#g_model.save(f'/home/tkeller/Outputs/nsteps{n_stepsX}_batch{nbatch}/g_model_{n_stepsX}_{nbatch}.h5')\n",
    "gan_model.save(f'/data-synology/tkeller/Outputs/nsteps{n_stepsX}_batch{nbatch}/gan_model_{n_stepsX}_{nbatch}.h5')\n",
    "gan_model.save(f'/data-synology/tkeller/Outputs/nsteps{n_stepsX}_batch{nbatch}/gan_model_{n_stepsX}_{nbatch}.keras')\n",
    "# Save the generator weights\n",
    "#g_model.save_weights(f'/home/tkeller/Outputs/nsteps{n_stepsX}_batch{nbatch}/g_model_weights_{n_stepsX}_{nbatch}.h5')\n",
    "\n",
    "gan_model.save_weights(f'/data-synology/tkeller/Outputs/nsteps{n_stepsX}_batch{nbatch}/gan_model_weights_{n_stepsX}_{nbatch}.h5')\n",
    "\n",
    "# save model as .keras\n",
    "gan_model.save(f'/data-synology/tkeller/Outputs/nsteps{n_stepsX}_batch{nbatch}/gan_model_{n_stepsX}_{nbatch}.keras')\n",
    "\n",
    "# load model\n",
    "#g_model = load_model(f'/home/tkeller/Outputs/nsteps{n_stepsX}_epochs{nepochs}/g_model_{n_stepsX}_{epochsX}.h5')\n",
    "\n",
    "# Load the saved generator weights\n",
    "#g_model.load_weights(f'/home/tkeller/Outputs/nsteps{n_stepsX}_epochs{nepochs}/g_model_weights_{n_stepsX}_{epochsX}.h5')\n",
    "\n",
    "print(\"Saved models to Outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZAc3uLSjLkB"
   },
   "source": [
    "Image generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_pUI_U_iMCS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, tar):\n",
    "\n",
    "    # print(test_input)\n",
    "    print(test_input.shape)\n",
    "    break()\n",
    "    prediction = model(test_input, training=True)\n",
    "\n",
    "    for j in range(5):\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        display_list = [test_input[j], tar[j], prediction[j]]\n",
    "        title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "        for i in range(3):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            plt.title(title[i])\n",
    "            # Getting the pixel values in the [0, 1] range to plot.\n",
    "            plt.imshow(display_list[i], cmap='gray', origin='lower')\n",
    "            plt.axis('off')\n",
    "        #os.getcwd()\n",
    "        #os.chdir(\"/home/tkeller/Outputs\")\n",
    "        plt.savefig(f'/data-synology/tkeller/Outputs/nsteps{n_stepsX}_batch{nbatch}/ExOutputImage_{n_stepsX}_{nbatch}_{j}.png', dpi=1000)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQ5dklV8qR44",
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir(\"/data-synology/anlee/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbrJALvFqnKn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loEpc84OO7zn"
   },
   "source": [
    "Create datatrain for Image generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtdDtTnpmgG8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "test_dataset = (test_input, test_output)\n",
    "#print(test_dataset)\n",
    "\n",
    "n_samples = 5\n",
    "random_indices = sample(range(len(test_input)), n_samples)\n",
    "print(random_indices)\n",
    "\n",
    "print(\"--------\")\n",
    "\n",
    "# pick 5 random samples from validation dataset\n",
    "random_input_paths_val = [val_input[idx] for idx in random_indices]\n",
    "random_output_paths_val = [val_output[idx] for idx in random_indices]\n",
    "\n",
    "val_input_images = []\n",
    "val_output_images = []\n",
    "\n",
    "#for input_path, output_path in zip(test_input, test_output):\n",
    "for input_path, output_path in zip(random_input_paths_val, random_output_paths_val):\n",
    "    # select random slice of 3D image: 0-191\n",
    "    #i = np.random.randint(0,191)\n",
    "    #print('slice i = '+ str(i))\n",
    "    input_image = load_image(input_path)\n",
    "    output_image = load_image(output_path)\n",
    "    val_input_images.append(np.expand_dims(input_image, -1))\n",
    "    val_output_images.append(np.expand_dims(output_image, -1))\n",
    "\n",
    "val_dataset = (val_input_images, val_output_images)\n",
    "#print(type(test_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsDz_D63j-uS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create np.array\n",
    "dataset = val_input_images, val_output_images\n",
    "val_dataset = np.array(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXu1G5ZlPDBS"
   },
   "source": [
    "Generate Fake Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xEL0OG43iNJk",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#for element in test_dataset[1]:\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mval_dataset\u001b[49m\n\u001b[1;32m      4\u001b[0m trainA2 \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m trainB2 \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#for element in test_dataset[1]:\n",
    "\n",
    "dataset = val_dataset\n",
    "trainA2 = dataset[0]\n",
    "trainB2 = dataset[1]\n",
    "\n",
    "generate_images(g_model, trainA2, trainB2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjvPWbxcL17j",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# move files\n",
    "shutil.move(\"/data-synology/tkeller/Outputs/plot_line_plot_loss.png\", f\"/data-synology/tkeller/Outputs/nsteps{n_stepsX}_batch{nbatch}/plot_line_plot_loss.png\")\n",
    "shutil.move(\"/data-synology/tkeller/Outputs/testDataDirName.csv\", f\"/data-synology/tkeller/Outputs/nsteps{n_stepsX}_batch{nbatch}/testDataDirName.csv\")\n",
    "shutil.move(f'/data-synology/tkeller/Outputs/modelCheckp_{n_stepsX}_{nbatch}', f'/data-synology/tkeller/Outputs/nsteps{n_stepsX}_batch{nbatch}/modelCheckp_{n_stepsX}_{nbatch}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "3I3_R8H2EoGI",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#print(gan_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "8wKO7GYJPVJJ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#weights = gan_model.get_weights()\n",
    "#print(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Qykakzd9PZpL",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#weights = g_model.get_weights()\n",
    "#print(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "4XJRahbeUWGv",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN1CR7+GzcjQFmlCHH6fZ8Y",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
